{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'image_path': '../testing-images/Unseen\\\\1.jpg', 'predicted_class': 'Pumpkin', 'raw_predictions': [-1.4929288625717163, -1.2911391258239746, -0.23721100389957428, 1.0044753551483154, -1.2316761016845703, -2.0316882133483887, 2.5895533561706543, 0.10909018665552139, -0.5758165121078491, 2.5659568309783936, 1.8187466859817505, -0.9016873240470886, 3.058994770050049, -3.222970485687256, -0.3659025728702545]}\n",
      "{'image_path': '../testing-images/Unseen\\\\2.jpg', 'predicted_class': 'Cabbage', 'raw_predictions': [-0.9408369660377502, -0.004756156355142593, -0.6222641468048096, 1.106011986732483, -0.018760809674859047, 2.407341241836548, -1.2085322141647339, -2.0814671516418457, 2.168921709060669, 0.04711754247546196, -0.4614936411380768, -0.9692177176475525, 1.4527355432510376, -1.2648720741271973, 0.5920839905738831]}\n",
      "{'image_path': '../testing-images/Unseen\\\\3.jpg', 'predicted_class': 'Brinjal', 'raw_predictions': [2.0876855850219727, -0.6450721621513367, 0.02028268575668335, 3.0922982692718506, 0.3764537274837494, -1.5912796258926392, 0.7602675557136536, -3.2316031455993652, 2.8981821537017822, 2.85534405708313, 2.195021390914917, -2.92307448387146, -1.3956786394119263, -3.128406047821045, -0.768933892250061]}\n",
      "{'image_path': '../testing-images/Unseen\\\\4.jpg', 'predicted_class': 'Bean', 'raw_predictions': [2.0379717350006104, -0.8675635457038879, 1.3721891641616821, -0.4209243655204773, 1.3477567434310913, 1.5214784145355225, -1.2905465364456177, -1.7863609790802002, -0.45086869597435, 0.7512387633323669, 0.01672983169555664, -0.7623093724250793, -1.6150691509246826, 0.2705749571323395, 0.16637539863586426]}\n",
      "{'image_path': '../testing-images/Unseen\\\\5.jpg', 'predicted_class': 'Cucumber', 'raw_predictions': [0.65814608335495, 4.008238315582275, -0.5086789727210999, -0.06353925913572311, 0.8947495818138123, 1.5063354969024658, -2.6500120162963867, -2.4323854446411133, 0.4603836238384247, 5.276557445526123, 0.6733794212341309, -3.647428274154663, -0.5693237781524658, -1.9632041454315186, -1.8543701171875]}\n",
      "{'image_path': '../testing-images/Unseen\\\\6.jpg', 'predicted_class': 'Tomato', 'raw_predictions': [-1.375564455986023, -0.45566123723983765, -0.9502884745597839, -0.7624306082725525, -0.8521600365638733, 1.06646728515625, -0.7341442704200745, 0.7134871482849121, 0.1916029304265976, -0.7023653984069824, -0.548741340637207, 0.43197301030158997, 2.1822962760925293, -0.8405386209487915, 2.62994122505188]}\n",
      "{'image_path': '../testing-images/Unseen\\\\BEET-ROOT.jpg', 'predicted_class': 'Capsicum', 'raw_predictions': [0.9648694396018982, -3.313460350036621, -1.6979457139968872, -0.15362155437469482, -1.8326236009597778, -4.945872783660889, 13.876392364501953, -1.3786817789077759, -4.061197280883789, -1.77456796169281, 0.7927608489990234, 0.838183581829071, 1.736334204673767, 0.6693660020828247, 0.4914950728416443]}\n",
      "{'image_path': '../testing-images/Unseen\\\\carrots-1.jpg', 'predicted_class': 'Tomato', 'raw_predictions': [-2.1714138984680176, -2.8512511253356934, -1.6800931692123413, -2.7728004455566406, -2.191530227661133, -3.516751289367676, 5.043097496032715, 3.566768169403076, -1.8801953792572021, -3.60512375831604, -0.463560551404953, 2.782167673110962, 0.005980327725410461, 1.655249834060669, 7.426413059234619]}\n",
      "{'image_path': '../testing-images/Unseen\\\\Carrot__40927.jpg', 'predicted_class': 'Carrot', 'raw_predictions': [-1.4218131303787231, -2.1554999351501465, -1.6942636966705322, -4.267001628875732, -1.9908686876296997, -3.787435531616211, 2.8247241973876953, 6.989011764526367, -3.0965144634246826, -2.5741496086120605, -0.46097490191459656, 3.573972225189209, -0.5372554659843445, 2.4462602138519287, 5.711787700653076]}\n",
      "{'image_path': '../testing-images/Unseen\\\\istockphoto-186018991-612x612.jpg', 'predicted_class': 'Capsicum', 'raw_predictions': [-3.203164577484131, -5.304905414581299, -2.4807729721069336, 0.3524094820022583, -3.6236820220947266, -7.106163501739502, 21.17399787902832, -3.504182815551758, -4.673224449157715, -2.2260963916778564, -0.26460975408554077, 3.1553990840911865, 8.627750396728516, -1.7005797624588013, 0.1414051055908203]}\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "import os\n",
    "from PIL import Image\n",
    "\n",
    "# Define the image directory\n",
    "image_directory = '../testing-images/Unseen'\n",
    "\n",
    "# Define class names\n",
    "class_names = ['Bean', 'Bitter_Gourd', 'Bottle_Gourd', 'Brinjal', 'Broccoli', 'Cabbage', 'Capsicum', 'Carrot', 'Cauliflower', 'Cucumber', 'Papaya', 'Potato', 'Pumpkin', 'Radish', 'Tomato']\n",
    "\n",
    "VGG_types = {\n",
    "    \"VGG16\": [\n",
    "        64,\n",
    "        64,\n",
    "        \"M\",\n",
    "        128,\n",
    "        128,\n",
    "        \"M\",\n",
    "        256,\n",
    "        256,\n",
    "        256,\n",
    "        \"M\",\n",
    "        512,\n",
    "        512,\n",
    "        512,\n",
    "        \"M\",\n",
    "        512,\n",
    "        512,\n",
    "        512,\n",
    "        \"M\",\n",
    "    ],\n",
    "}\n",
    "\n",
    "class VGG_net(nn.Module):\n",
    "    def __init__(self, input_channels, num_classes=2):\n",
    "        super(VGG_net, self).__init__()\n",
    "        self.in_channels = input_channels\n",
    "        self.conv_layers = self.create_conv_layers(VGG_types['VGG16'])  # create our conv layers\n",
    "        self.fcs = nn.Sequential(\n",
    "            nn.Linear(512 * 7 * 7, 4096),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=0.5),\n",
    "            nn.Linear(4096, 4096),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=0.5),\n",
    "            nn.Linear(4096, num_classes)  # sizeInputImage = 224, divided by num Maxpool : 224 / 2⁷ = 7\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv_layers(x)\n",
    "        x = x.reshape(x.shape[0], -1)  # flatten our convlayers\n",
    "        x = self.fcs(x)\n",
    "        return x\n",
    "\n",
    "    def create_conv_layers(self, architecture):\n",
    "        layers = []\n",
    "        in_channels = self.in_channels\n",
    "        for layer in architecture:\n",
    "            if type(layer) is int:\n",
    "                out_channels = layer\n",
    "                layers += [nn.Conv2d(in_channels=in_channels, out_channels=out_channels,\n",
    "                                    kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),\n",
    "                            nn.BatchNorm2d(layer),\n",
    "                            nn.ReLU()]\n",
    "                in_channels = layer  # for the next iteration\n",
    "            elif layer == 'M':\n",
    "                layers += [nn.MaxPool2d(kernel_size=(2, 2), stride=(2, 2))]\n",
    "        return nn.Sequential(*layers,)\n",
    "\n",
    "\n",
    "# Load the trained model\n",
    "model_path = '../fun2/VGG16_torch_model_epoch40.pt'\n",
    "model = VGG_net(input_channels=3, num_classes=len(class_names))\n",
    "model.load_state_dict(torch.load(model_path, map_location=torch.device('cpu')))\n",
    "model.eval()\n",
    "\n",
    "# Define the transformation for input images\n",
    "data_transform = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "# Function to predict and return the results along with raw predictions\n",
    "def predict_image(image_path, model, class_names):\n",
    "    try:\n",
    "        image = Image.open(image_path)\n",
    "        image_tensor = data_transform(image).unsqueeze(0)\n",
    "\n",
    "        # Check if GPU is available\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "        # Move the input tensor to the appropriate device\n",
    "        image_tensor = image_tensor.to(device)\n",
    "\n",
    "        # Perform inference\n",
    "        with torch.no_grad():\n",
    "            model.eval()\n",
    "            output = model(image_tensor)\n",
    "\n",
    "        _, predicted_class = torch.max(output, 1)\n",
    "\n",
    "        return {\n",
    "            'image_path': image_path,\n",
    "            'predicted_class': class_names[predicted_class.item()],\n",
    "            'raw_predictions': output.squeeze().tolist()\n",
    "        }\n",
    "\n",
    "    except Exception as e:\n",
    "        return {\n",
    "            'image_path': image_path,\n",
    "            'error': str(e)\n",
    "        }\n",
    "\n",
    "# Test the model on all files in the specified directory and print the results\n",
    "for filename in os.listdir(image_directory):\n",
    "    image_path = os.path.join(image_directory, filename)\n",
    "    result = predict_image(image_path, model, class_names)\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'image_path': '../testing-images/Unseen\\\\1.jpg', 'predicted_class': 'Cucumber'}\n",
      "{'image_path': '../testing-images/Unseen\\\\2.jpg', 'predicted_class': 'Cabbage'}\n",
      "{'image_path': '../testing-images/Unseen\\\\3.jpg', 'predicted_class': 'Cucumber'}\n",
      "{'image_path': '../testing-images/Unseen\\\\4.jpg', 'predicted_class': 'Cucumber'}\n",
      "{'image_path': '../testing-images/Unseen\\\\5.jpg', 'predicted_class': 'Cucumber'}\n",
      "{'image_path': '../testing-images/Unseen\\\\6.jpg', 'predicted_class': 'Tomato'}\n",
      "{'image_path': '../testing-images/Unseen\\\\BEET-ROOT.jpg', 'predicted_class': 'Capsicum'}\n",
      "{'image_path': '../testing-images/Unseen\\\\bottlegourd-approx-600-g-900-g-product.webp', 'predicted_class': 'Cucumber'}\n",
      "{'image_path': '../testing-images/Unseen\\\\Bottle_Gourd.jpg', 'predicted_class': 'Cucumber'}\n",
      "{'image_path': '../testing-images/Unseen\\\\brinjal-black-giant-round-28.webp', 'predicted_class': 'Potato'}\n",
      "{'image_path': '../testing-images/Unseen\\\\brinjal_1_-Copy_2048x2048.webp', 'predicted_class': 'Capsicum'}\n",
      "{'image_path': '../testing-images/Unseen\\\\carrots-1.jpg', 'predicted_class': 'Tomato'}\n",
      "{'image_path': '../testing-images/Unseen\\\\Carrot__40927.jpg', 'predicted_class': 'Carrot'}\n",
      "{'image_path': '../testing-images/Unseen\\\\Cooking-Melon-Kekiri-1.jpg', 'predicted_class': 'Capsicum'}\n",
      "{'image_path': '../testing-images/Unseen\\\\E72A0178.jpg', 'predicted_class': 'Cucumber'}\n",
      "{'image_path': '../testing-images/Unseen\\\\green-beans-square.jpeg', 'predicted_class': 'Cabbage'}\n",
      "{'image_path': '../testing-images/Unseen\\\\istockphoto-186018991-612x612.jpg', 'predicted_class': 'Capsicum'}\n",
      "{'image_path': '../testing-images/Unseen\\\\karela-bitter-gourd-300g.jpg', 'predicted_class': 'Broccoli'}\n",
      "{'image_path': '../testing-images/Unseen\\\\kidney-beans-.webp', 'predicted_class': 'Carrot'}\n",
      "{'image_path': '../testing-images/Unseen\\\\papaya.webp', 'predicted_class': 'Potato'}\n",
      "{'image_path': '../testing-images/Unseen\\\\product_kakiri_533_1.webp', 'predicted_class': 'Potato'}\n",
      "{'image_path': '../testing-images/Unseen\\\\Ripepapaya.webp', 'predicted_class': 'Capsicum'}\n",
      "{'image_path': '../testing-images/Unseen\\\\sdz360_F_389858598_bt.jpg', 'predicted_class': 'Bitter_Gourd'}\n",
      "{'image_path': '../testing-images/Unseen\\\\Solanum_melongena__(1).jpg', 'predicted_class': 'Bean'}\n",
      "{'image_path': '../testing-images/Unseen\\\\solated-white-clipping-path.webp', 'predicted_class': 'Capsicum'}\n",
      "{'image_path': '../testing-images/Unseen\\\\SriLanka_kakiri1606231285916.webp', 'predicted_class': 'Capsicum'}\n",
      "{'image_path': '../testing-images/Unseen\\\\uti182035936-612x612.jpg', 'predicted_class': 'Bean'}\n",
      "{'image_path': '../testing-images/Unseen\\\\yu250-papaya-.webp', 'predicted_class': 'Capsicum'}\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "import os\n",
    "from PIL import Image\n",
    "\n",
    "# Define the image directory\n",
    "image_directory = '../testing-images/Unseen'\n",
    "\n",
    "# Define class names\n",
    "class_names = ['Bean', 'Bitter_Gourd', 'Bottle_Gourd', 'Brinjal', 'Broccoli', 'Cabbage', 'Capsicum', 'Carrot', 'Cauliflower', 'Cucumber', 'Papaya', 'Potato', 'Pumpkin', 'Radish', 'Tomato']\n",
    "\n",
    "VGG_types = {\n",
    "    \"VGG16\": [\n",
    "        64,\n",
    "        64,\n",
    "        \"M\",\n",
    "        128,\n",
    "        128,\n",
    "        \"M\",\n",
    "        256,\n",
    "        256,\n",
    "        256,\n",
    "        \"M\",\n",
    "        512,\n",
    "        512,\n",
    "        512,\n",
    "        \"M\",\n",
    "        512,\n",
    "        512,\n",
    "        512,\n",
    "        \"M\",\n",
    "    ],\n",
    "}\n",
    "\n",
    "class VGG_net(nn.Module):\n",
    "    def __init__(self, input_channels, num_classes=2):\n",
    "        super(VGG_net, self).__init__()\n",
    "        self.in_channels = input_channels\n",
    "        self.conv_layers = self.create_conv_layers(VGG_types['VGG16'])  # create our conv layers\n",
    "        self.fcs = nn.Sequential(\n",
    "            nn.Linear(512 * 7 * 7, 4096),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=0.5),\n",
    "            nn.Linear(4096, 4096),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=0.5),\n",
    "            nn.Linear(4096, num_classes)  # sizeInputImage = 224, divided by num Maxpool : 224 / 2⁷ = 7\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv_layers(x)\n",
    "        x = x.reshape(x.shape[0], -1)  # flatten our convlayers\n",
    "        x = self.fcs(x)\n",
    "        return x\n",
    "\n",
    "    def create_conv_layers(self, architecture):\n",
    "        layers = []\n",
    "        in_channels = self.in_channels\n",
    "        for layer in architecture:\n",
    "            if type(layer) is int:\n",
    "                out_channels = layer\n",
    "                layers += [nn.Conv2d(in_channels=in_channels, out_channels=out_channels,\n",
    "                                    kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),\n",
    "                            nn.BatchNorm2d(layer),\n",
    "                            nn.ReLU()]\n",
    "                in_channels = layer  # for the next iteration\n",
    "            elif layer == 'M':\n",
    "                layers += [nn.MaxPool2d(kernel_size=(2, 2), stride=(2, 2))]\n",
    "        return nn.Sequential(*layers,)\n",
    "\n",
    "\n",
    "# Load the trained model\n",
    "model_path = '../fun2/VGG16_torch_model_epoch40.pt'\n",
    "model = VGG_net(input_channels=3, num_classes=len(class_names))\n",
    "model.load_state_dict(torch.load(model_path, map_location=torch.device('cpu')))\n",
    "model.eval()\n",
    "\n",
    "# Define the transformation for input images\n",
    "data_transform = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "# Function to predict and return the results along with raw predictions\n",
    "def predict_image(image_path, model, class_names):\n",
    "    try:\n",
    "        image = Image.open(image_path)\n",
    "        image_tensor = data_transform(image).unsqueeze(0)\n",
    "\n",
    "        # Check if GPU is available\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "        # Move the input tensor to the appropriate device\n",
    "        image_tensor = image_tensor.to(device)\n",
    "\n",
    "        # Perform inference\n",
    "        with torch.no_grad():\n",
    "            model.eval()\n",
    "            output = model(image_tensor)\n",
    "\n",
    "        _, predicted_class = torch.max(output, 1)\n",
    "\n",
    "        raw_predictions = output.squeeze().tolist()\n",
    "\n",
    "        # Check if raw prediction for Cucumber is greater than 1.5\n",
    "        if raw_predictions[class_names.index('Cucumber')] > 0.7:\n",
    "            return {\n",
    "                'image_path': image_path,\n",
    "                'predicted_class': 'Cucumber',\n",
    "            }\n",
    "        else:\n",
    "            return {\n",
    "                'image_path': image_path,\n",
    "                'predicted_class': class_names[predicted_class.item()],\n",
    "            }\n",
    "\n",
    "    except Exception as e:\n",
    "        return {\n",
    "            'image_path': image_path,\n",
    "            'error': str(e)\n",
    "        }\n",
    "\n",
    "# Test the model on all files in the specified directory and print the results\n",
    "for filename in os.listdir(image_directory):\n",
    "    image_path = os.path.join(image_directory, filename)\n",
    "    result = predict_image(image_path, model, class_names)\n",
    "    print(result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cucumber-dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
